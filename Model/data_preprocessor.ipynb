{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "import xml.etree.cElementTree as et\n",
    "import html\n",
    "import matplotlib.pyplot as plt\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XML data structure of the files :\n",
    "```xml\n",
    "<WhiteboardCaptureSession>\n",
    "    <General> General Information </General>\n",
    "    <Transcription>\n",
    "        <Text> The text corresponding to the strokes </Text>\n",
    "        <TextLine id=\"a01-001z-01\" text=\"By Trevor Williams. A move\">\n",
    "              <Word id=\"a01-001z-01-01\" text=\"By\">\n",
    "                <Char id=\"a01-001z-01-01-01\" text=\"B\"/>\n",
    "                <Char id=\"a01-001z-01-01-02\" text=\"y\"/>\n",
    "              </Word>\n",
    "              .................. similar details for every word in the textline\n",
    "        </TextLine>\n",
    "        .................. similar details for every textline in the text\n",
    "    </Transcription>\n",
    "    <WhiteboardDescription>\n",
    "        <SensorLocation corner=\"top_left\"/>\n",
    "        <DiagonallyOppositeCoords x=\"6912\" y=\"8798\"/>\n",
    "        <VerticallyOppositeCoords x=\"214\" y=\"8878\"/>\n",
    "        <HorizontallyOppositeCoords x=\"7038\" y=\"196\"/>\n",
    "    </WhiteboardDescription>\n",
    "    <StrokeSet>\n",
    "        <Stroke colour=\"black\" start_time=\"13090871.35\" end_time=\"13090871.78\">\n",
    "            <Point x=\"895\" y=\"992\" time=\"13090871.35\"/>\n",
    "            ............ similar details of points in the stroke\n",
    "        </Stroke>\n",
    "        ............... similar details of stroke in the StrokeSet\n",
    "    </StrokeSet>\n",
    "</WhiteboardCaptureSession>\n",
    "```\n",
    "So firstly, we need to parse the data from the xml and arrange them in a proper format : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(point_1, point_2):\n",
    "    return np.sqrt(np.sum(np.square(point_1 - point_2)))\n",
    "\n",
    "# To make sure that the points are not too far away, and if they are, remove that piece of data\n",
    "def clear_points(points):\n",
    "    plot = False\n",
    "    points_to_remove = set()\n",
    "    for i in range(1, len(points) - 1):\n",
    "        p1, p2, p3 = points[i - 1: i + 2, :2]\n",
    "        \n",
    "        # sum of distance between 3 sequential points in the stroke\n",
    "        dis = distance(p1, p2) + distance(p2, p3)\n",
    "        if dis > 1500:\n",
    "            points_to_remove.add(i)\n",
    "        \n",
    "    valid_pts = []\n",
    "    for i in range(len(points)):\n",
    "        if i not in points_to_remove:\n",
    "            valid_pts.append(points[i])\n",
    "    \n",
    "    return np.array(valid_pts)\n",
    "\n",
    "# separates the points of the stroke into separate groups based on the distance between the points\n",
    "# interprete them as separate strokes\n",
    "# SHAPES : init = (30, 3) , zip = (14, 30) , final = [(14, 3), (16, 3)]\n",
    "def separate(points):\n",
    "    seps = []\n",
    "    for i in range(0, len(points) - 1):\n",
    "        if distance(points[i], points[i+1]) > 600:\n",
    "            seps.append(i+1)\n",
    "    z = zip([0] + seps, seps + [len(points)])\n",
    "    \n",
    "    final = [points[b:e] for b, e in zip([0] + seps, seps + [len(points)])]\n",
    "    \n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '<NULL>', 1: ' ', 2: 'a', 3: 'b', 4: 'c', 5: 'd', 6: 'e', 7: 'f', 8: 'g', 9: 'h', 10: 'i', 11: 'j', 12: 'k', 13: 'l', 14: 'm', 15: 'n', 16: 'o', 17: 'p', 18: 'q', 19: 'r', 20: 's', 21: 't', 22: 'u', 23: 'v', 24: 'w', 25: 'x', 26: 'y', 27: 'z', 28: 'A', 29: 'B', 30: 'C', 31: 'D', 32: 'E', 33: 'F', 34: 'G', 35: 'H', 36: 'I', 37: 'J', 38: 'K', 39: 'L', 40: 'M', 41: 'N', 42: 'O', 43: 'P', 44: 'Q', 45: 'R', 46: 'S', 47: 'T', 48: 'U', 49: 'V', 50: 'W', 51: 'X', 52: 'Y', 53: 'Z', 54: '1', 55: '2', 56: '3', 57: '4', 58: '5', 59: '6', 60: '7', 61: '8', 62: '9', 63: '0', 64: '-', 65: \"'\", 66: '#', 67: ')', 68: '/', 69: '!', 70: '\"', 71: ',', 72: ';', 73: '?', 74: '&', 75: ':', 76: '.', 77: '(', 78: '+', 79: '%'}\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "characters = set()\n",
    "\n",
    "# Surf through all the files in the dataset\n",
    "for root, dirs, files in os.walk('.\\data'):\n",
    "    for file in files:\n",
    "        if file.split('.')[-1] == 'xml':\n",
    "            raw_data = et.parse(root + \"\\\\\" + file).getroot()\n",
    "            transcription = raw_data.findall(\"Transcription\")  # gives us a list, transcription object at transcription[0]\n",
    "            strokeset = raw_data.findall('StrokeSet') # gives us a list, strokeset object at strokeset[0]\n",
    "            \n",
    "            if not transcription:\n",
    "                continue\n",
    "            \n",
    "            # get the text\n",
    "            ascii_text = [html.unescape(line.get('text')) for line in transcription[0].findall(\"TextLine\")]\n",
    "            \n",
    "            # get every stroke as a list of points in the strokeset and adds it to a list\n",
    "            strokes_init = [stroke.findall('Point') for stroke in strokeset[0].findall('Stroke')] \n",
    "            \n",
    "            strokes = []\n",
    "            midpoints = []\n",
    "            chars = ' abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ1234567890-\\'#)/!\",;?&:.(+%'\n",
    "            charlist = [char for char in chars]\n",
    "            \n",
    "            for single_stroke in strokes_init:\n",
    "                \n",
    "                # returns the list of (x, y, eos) coordinates for a single stroke\n",
    "                coords = np.array([[int(point.get('x')), int(point.get('y')), 0] for point in single_stroke])\n",
    "                \n",
    "                # assign the last element of the last point as 1 to indicate end of stroke\n",
    "                coords[-1, 2] = 1\n",
    "                \n",
    "                # Now, we need to filter the data\n",
    "                coords = clear_points(coords)\n",
    "                \n",
    "                if len(coords) == 0:\n",
    "                    continue\n",
    "                    \n",
    "                seps = separate(coords)\n",
    "                \n",
    "                for stroke in seps:\n",
    "                    # check for single points as strokes\n",
    "                    if len(seps) > 1 and len(stroke) == 1:\n",
    "                        continue\n",
    "                    stroke[-1, 2] = 1\n",
    "                    \n",
    "                    # get the maximum value of x and y coordinates for each stroke\n",
    "                    xmax, ymax = max(stroke, key = lambda x: x[0])[0], max(stroke, key = lambda x: x[1])[1]\n",
    "                    \n",
    "                    # get the minimum value of x and y coordinates for each stroke\n",
    "                    xmin, ymin = min(stroke, key = lambda x : x[0])[0], min(stroke, key = lambda x: x[1])[1]\n",
    "                    \n",
    "                    strokes.append(stroke)\n",
    "                    \n",
    "                    # calculate the midpoints for each stroke\n",
    "                    midpoints.append([(xmax + xmin)/2, (ymax + ymin)/2])\n",
    "            \n",
    "            # for every point in a single strokeset\n",
    "            distances = [-abs(p1[0] - p2[0]) + abs(p1[1] - p2[1]) for p1, p2 in zip(midpoints, midpoints[1:])]\n",
    "            splits = sorted(np.argsort(distances)[:len(ascii_text) - 1] + 1)\n",
    "            lines = []\n",
    "            \n",
    "            for b, e in zip([0] + splits, splits + [len(strokes)]):\n",
    "                lines.append([point for stroke in strokes[b:e] for point in stroke])\n",
    "            \n",
    "            data.append((ascii_text, lines))\n",
    "            \n",
    "# we assign indexes to every character in the dataset          \n",
    "translation = {0 : '<NULL>'}\n",
    "i = 1\n",
    "for char in charlist:\n",
    "    translation[i] = char\n",
    "    i += 1\n",
    "\n",
    "print(translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(text, translation):\n",
    "    final_list = []\n",
    "    for i in range(len(text)):\n",
    "        try:\n",
    "            final_list.append(translation[text[i]])\n",
    "        except KeyError:\n",
    "            # keep adding new characters to the translation\n",
    "            translation[text[i]] = len(translation)\n",
    "            final_list.append(translation[text[i]])\n",
    "    return translation, final_list\n",
    "    \n",
    "dataset = []\n",
    "labels = []\n",
    "\n",
    "for texts, lines in data:\n",
    "    for text, line in zip(texts, lines):\n",
    "        line = np.array(line, dtype = np.float32)\n",
    "        # shift the coordinates\n",
    "        line[:,0] = line[:,0] - np.min(line[:,0])\n",
    "        line[:,1] = line[:,1] - np.min(line[:,1])\n",
    "        \n",
    "        # now add the lines to the dataset\n",
    "        dataset.append(line)\n",
    "        \n",
    "        # get the indexes just in case if we need one hots somewhere in the future\n",
    "        translation , final_list= translate(text, translation)\n",
    "        labels.append(final_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_data = np.concatenate(dataset, axis=0)\n",
    "y_std = np.std(whole_data[:, 1])\n",
    "norm_data = []\n",
    "# normalize the data\n",
    "for line in dataset:\n",
    "    line[:, :2] /= y_std\n",
    "    norm_data.append(line)\n",
    "dataset = norm_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that all the data is parsed successfully, we save the data\n",
    "try:\n",
    "    os.makedirs('data_parsed')\n",
    "except:\n",
    "    pass\n",
    "np.save('data_parsed/dataset', np.array(dataset))\n",
    "np.save('data_parsed/labels', np.array(labels))\n",
    "with open('data_parsed/translation.pkl', 'wb') as translation_file:\n",
    "    pickle.dump(translation, translation_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
