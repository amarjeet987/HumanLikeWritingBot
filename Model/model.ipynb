{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from collections import namedtuple\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from matplotlib import style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_path():\n",
    "    idx = 0\n",
    "    path = os.path.join('models', 'model-{}')\n",
    "    while os.path.exists(path.format(idx)):\n",
    "        idx += 1\n",
    "    path = path.format(idx)\n",
    "    os.makedirs(os.path.join(path, 'models'))\n",
    "    os.makedirs(os.path.join(path, 'backup'))\n",
    "    for file in filter(lambda x: x.endswith('.py'), os.listdir('.')):\n",
    "        shutil.copy2(file, os.path.join(path, 'backup'))\n",
    "    return path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Set the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 256\n",
    "batch_size = 64\n",
    "num_epoch = 30\n",
    "window_mixtures = 10\n",
    "output_mixtures = 20\n",
    "lstm_layers = 3\n",
    "units_per_layer = 400\n",
    "# change this to the path where the model was saved if you wish to continue trainng a half trained model\n",
    "restore_model = None \n",
    "batches_per_epoch = 50\n",
    "epsilon = 1e-8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Define the window layer<br/>\n",
    "\n",
    "**a) Calculate the parameters**\n",
    "<br/><br/>\n",
    "$(\\alpha', \\beta', \\kappa') = W_{h^1p}h_t + b_p$\n",
    "<br/><br/>\n",
    "$\\alpha = exp(\\alpha')$\n",
    "<br/><br/>\n",
    "$\\beta = exp(\\beta')$\n",
    "<br/><br/>\n",
    "$\\kappa = k_{t-1} + exp(\\kappa')$\n",
    "<br/><br/>\n",
    "\n",
    "**b) Calculate phi**\n",
    "<br/><br/>\n",
    "$\\phi(t, u) = \\sum_{k=1}^{K} {\\alpha_t^k exp(-\\beta^k(\\kappa^k - u)^2)}$\n",
    "<br/><br/>\n",
    "**c) Now, finally, calculate the window weights**\n",
    "<br/><br/>\n",
    "$w_t = \\sum_{u=1}^{U}{\\phi(t, u)c_u}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WindowLayer(object):\n",
    "    def __init__(self, num_mixtures, sequence, num_letters):\n",
    "        self.sequence = sequence  # one-hot encoded sequence of characters -- [batch_size, length, num_letters]\n",
    "        self.seq_len = tf.shape(sequence)[1]\n",
    "        self.num_mixtures = num_mixtures\n",
    "        self.num_letters = num_letters\n",
    "        self.u_range = -tf.expand_dims(tf.expand_dims(tf.range(0., tf.cast(self.seq_len, dtype=tf.float32)), axis=0),\n",
    "                                       axis=0)\n",
    "\n",
    "    def __call__(self, inputs, k, reuse=None):\n",
    "        with tf.variable_scope('window', reuse=reuse):\n",
    "            alpha = tf.layers.dense(inputs, self.num_mixtures, activation=tf.exp,\n",
    "                                    kernel_initializer=tf.truncated_normal_initializer(stddev=0.075), name='alpha')\n",
    "            beta = tf.layers.dense(inputs, self.num_mixtures, activation=tf.exp,\n",
    "                                   kernel_initializer=tf.truncated_normal_initializer(stddev=0.075), name='beta')\n",
    "            kappa = tf.layers.dense(inputs, self.num_mixtures, activation=tf.exp,\n",
    "                                    kernel_initializer=tf.truncated_normal_initializer(stddev=0.075), name='kappa')\n",
    "\n",
    "            a = tf.expand_dims(alpha, axis=2)\n",
    "            b = tf.expand_dims(beta, axis=2)\n",
    "            k = tf.expand_dims(k + kappa, axis=2)\n",
    "\n",
    "            phi = tf.exp(-np.square(self.u_range + k) * b) * a  # [batch_size, mixtures, length]\n",
    "            phi = tf.reduce_sum(phi, axis=1, keepdims=True)  # [batch_size, 1, length]\n",
    "\n",
    "            # whether or not network finished generating sequence\n",
    "            finish = tf.cast(phi[:, 0, -1] > tf.reduce_max(phi[:, 0, :-1], axis=1), tf.float32)\n",
    "\n",
    "            return tf.squeeze(tf.matmul(phi, self.sequence), axis=1), \\\n",
    "                   tf.squeeze(k, axis=2), \\\n",
    "                   tf.squeeze(phi, axis=1), \\\n",
    "                   tf.expand_dims(finish, axis=1)\n",
    "\n",
    "    @property\n",
    "    def output_size(self):\n",
    "        return [self.num_letters, self.num_mixtures, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Define the Mixture layer<br/>\n",
    "\n",
    "**Calculate the parameters**\n",
    "\n",
    "$y_t' = (e_t',(\\pi_t'^j, \\mu_t'^j, \\sigma_t'^j, \\rho_t'^j)_{m=1}^{M}) = b_y + \\sum_{n=1}^{N}{W_{h^ny}h_t^n}$\n",
    "<br/><br/>\n",
    "$e_t = \\frac{1}{1 + exp(e_t')}\\,\\,\\,\\,\\,(sigmoid)$\n",
    "<br/><br/>\n",
    "$\\pi_t^j = \\frac{exp(\\pi_t'^j)}{\\sum_{j'=1}^{M}{exp(\\pi_t'^j)}}\\,\\,\\,\\,\\,(softmax)$\n",
    "<br/><br/>\n",
    "$\\mu_t^j = \\mu_t'^j$\n",
    "<br/><br/>\n",
    "$\\sigma_t^j = exp(\\sigma_t'^j)$\n",
    "<br/><br/>\n",
    "$\\rho_t^j = \\tanh(\\rho_t'^j)$\n",
    "<br/><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixtureLayer(object):\n",
    "    def __init__(self, input_size, output_size, num_mixtures):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.num_mixtures = num_mixtures\n",
    "\n",
    "    def __call__(self, inputs, bias=0., reuse=None):\n",
    "        with tf.variable_scope('mixture_output', reuse=reuse):\n",
    "            e = tf.layers.dense(inputs, 1,\n",
    "                                kernel_initializer=tf.truncated_normal_initializer(stddev=0.075), name='e')\n",
    "            pi = tf.layers.dense(inputs, self.num_mixtures,\n",
    "                                 kernel_initializer=tf.truncated_normal_initializer(stddev=0.075), name='pi')\n",
    "            mu1 = tf.layers.dense(inputs, self.num_mixtures,\n",
    "                                  kernel_initializer=tf.truncated_normal_initializer(stddev=0.075), name='mu1')\n",
    "            mu2 = tf.layers.dense(inputs, self.num_mixtures,\n",
    "                                  kernel_initializer=tf.truncated_normal_initializer(stddev=0.075), name='mu2')\n",
    "            std1 = tf.layers.dense(inputs, self.num_mixtures,\n",
    "                                   kernel_initializer=tf.truncated_normal_initializer(stddev=0.075), name='std1')\n",
    "            std2 = tf.layers.dense(inputs, self.num_mixtures,\n",
    "                                   kernel_initializer=tf.truncated_normal_initializer(stddev=0.075), name='std2')\n",
    "            rho = tf.layers.dense(inputs, self.num_mixtures,\n",
    "                                  kernel_initializer=tf.truncated_normal_initializer(stddev=0.075), name='rho')\n",
    "\n",
    "            return tf.nn.sigmoid(e), \\\n",
    "                   tf.nn.softmax(pi * (1. + bias)), \\\n",
    "                   mu1, mu2, \\\n",
    "                   tf.exp(std1 - bias), tf.exp(std2 - bias), \\\n",
    "                   tf.nn.tanh(rho)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Define the RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(tf.nn.rnn_cell.RNNCell):\n",
    "    def __init__(self, layers, num_units, input_size, num_letters, batch_size, window_layer):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.layers = layers\n",
    "        self.num_units = num_units\n",
    "        self.input_size = input_size\n",
    "        self.num_letters = num_letters\n",
    "        self.window_layer = window_layer\n",
    "        self.last_phi = None\n",
    "\n",
    "        with tf.variable_scope('rnn', reuse=None):\n",
    "            self.lstms = [tf.nn.rnn_cell.LSTMCell(num_units)\n",
    "                          for _ in range(layers)]\n",
    "            self.states = [tf.Variable(tf.zeros([batch_size, s]), trainable=False)\n",
    "                           for s in self.state_size]\n",
    "\n",
    "            self.zero_states = tf.group(*[sp.assign(sc)\n",
    "                                          for sp, sc in zip(self.states,\n",
    "                                                            self.zero_state(batch_size, dtype=tf.float32))])\n",
    "\n",
    "    @property\n",
    "    def state_size(self):\n",
    "        return [self.num_units] * self.layers * 2 + self.window_layer.output_size\n",
    "\n",
    "    @property\n",
    "    def output_size(self):\n",
    "        return [self.num_units]\n",
    "\n",
    "    def call(self, inputs, state, **kwargs):\n",
    "        # state[-3] --> window\n",
    "        # state[-2] --> k\n",
    "        # state[-1] --> finish\n",
    "        # state[2n] --> h\n",
    "        # state[2n+1] --> c\n",
    "        window, k, finish = state[-3:]\n",
    "        output_state = []\n",
    "        prev_output = []\n",
    "\n",
    "        for layer in range(self.layers):\n",
    "            x = tf.concat([inputs, window] + prev_output, axis=1)\n",
    "            with tf.variable_scope('lstm_{}'.format(layer)):\n",
    "                output, s = self.lstms[layer](x, tf.nn.rnn_cell.LSTMStateTuple(state[2 * layer],\n",
    "                                                                               state[2 * layer + 1]))\n",
    "                prev_output = [output]\n",
    "            output_state += [*s]\n",
    "\n",
    "            if layer == 0:\n",
    "                window, k, self.last_phi, finish = self.window_layer(output, k)\n",
    "\n",
    "        return output, output_state + [window, k, finish]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Loss function\n",
    "* Calculate the probability of next input being $x_{t+1}$ for output vector $y_t$ is\n",
    "<br/><br/>\n",
    "$P(x_{t+1}|y_t) = \\sum_{j=1}^{M}\\pi_t^jN(x_{t+1}|\\mu_t^j, \\sigma_t^j, \\rho_t^j) \n",
    "\\begin{cases}\n",
    "  e_t & \\text{if }(x_{t+1})_3 = 1 \\\\    \n",
    "  1-e_t & \\text{otherwise}\\\\  \n",
    "\\end{cases}$\n",
    "<br/><br/>\n",
    "where\n",
    "<br/><br/>\n",
    "$N(x, \\mu, \\sigma, \\rho) = \\frac{1}{2\\pi\\sigma_1\\sigma_2\\sqrt{1 - \\rho^2}}exp\\,(\\frac{-Z}{2(1 - \\rho^2)})$\n",
    "<br/><br/>\n",
    "$Z = \\frac{(x_1 - \\mu_1)^2}{\\sigma_1^2} + \\frac{(x_2 - \\mu_2)^2}{\\sigma_2^2} \n",
    "+ \\frac{2\\rho(x_1 - \\mu_1)(x_2 - \\mu_2)}{\\sigma_1\\sigma_2}$\n",
    "<br/><br/>\n",
    "* The loss function is given by taking it's negative logarithm and adding them up over all timesteps\n",
    "<br/><br/>\n",
    "$L(x) = \\sum_{t=1}^{T}-\\log({\\sum_{j=1}^{M}\\pi_t^jN(x_{t+1}|\\mu_t^j, \\sigma_t^j, \\rho_t^j)}) - \n",
    "\\begin{cases}\n",
    "    log(e_t) & \\text{if }(x_{t+1})_3 = 1 \\\\    \n",
    "    log(1-e_t) & \\text{otherwise}\\\\  \n",
    "\\end{cases}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph(num_letters, batch_size,\n",
    "                 num_units=400, lstm_layers=3,\n",
    "                 window_mixtures=10, output_mixtures=20):\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "        coordinates = tf.placeholder(tf.float32, shape=[None, None, 3])\n",
    "        sequence = tf.placeholder(tf.float32, shape=[None, None, num_letters])\n",
    "        reset = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "        bias = tf.placeholder_with_default(tf.zeros(shape=[]), shape=[])\n",
    "\n",
    "        def create_model(generate=None):\n",
    "            in_coords = coordinates[:, :-1, :]\n",
    "            out_coords = coordinates[:, 1:, :]\n",
    "\n",
    "            _batch_size = 1 if generate else batch_size\n",
    "            if generate:\n",
    "                in_coords = coordinates\n",
    "\n",
    "            with tf.variable_scope('model', reuse=generate):\n",
    "                window = WindowLayer(num_mixtures=window_mixtures, sequence=sequence, num_letters=num_letters)\n",
    "\n",
    "                rnn_model = RNNModel(layers=lstm_layers, num_units=num_units,\n",
    "                                     input_size=3, num_letters=num_letters,\n",
    "                                     window_layer=window, batch_size=_batch_size)\n",
    "\n",
    "                reset_states = tf.group(*[state.assign(state * reset)\n",
    "                                          for state in rnn_model.states])\n",
    "\n",
    "                outs, states = tf.nn.dynamic_rnn(rnn_model, in_coords,\n",
    "                                                 initial_state=rnn_model.states)\n",
    "\n",
    "                output_layer = MixtureLayer(input_size=num_units, output_size=2,\n",
    "                                            num_mixtures=output_mixtures)\n",
    "\n",
    "                with tf.control_dependencies([sp.assign(sc) for sp, sc in zip(rnn_model.states, states)]):\n",
    "                    with tf.name_scope('prediction'):\n",
    "                        outs = tf.reshape(outs, [-1, num_units])\n",
    "                        e, pi, mu1, mu2, std1, std2, rho = output_layer(outs, bias)\n",
    "\n",
    "                    with tf.name_scope('loss'):\n",
    "                        coords = tf.reshape(out_coords, [-1, 3])\n",
    "                        xs, ys, es = tf.unstack(tf.expand_dims(coords, axis=2), axis=1)\n",
    "\n",
    "                        mrho = 1 - tf.square(rho)\n",
    "                        xms = (xs - mu1) / std1\n",
    "                        yms = (ys - mu2) / std2\n",
    "                        z = tf.square(xms) + tf.square(yms) - 2. * rho * xms * yms\n",
    "                        n = 1. / (2. * np.pi * std1 * std2 * tf.sqrt(mrho)) * tf.exp(-z / (2. * mrho))\n",
    "                        ep = es * e + (1. - es) * (1. - e)\n",
    "                        rp = tf.reduce_sum(pi * n, axis=1)\n",
    "\n",
    "                        loss = tf.reduce_mean(-tf.log(rp + epsilon) - tf.log(ep + epsilon))\n",
    "\n",
    "                    if generate:\n",
    "                        # save params for easier model loading and prediction\n",
    "                        for param in [('coordinates', coordinates),\n",
    "                                      ('sequence', sequence),\n",
    "                                      ('bias', bias),\n",
    "                                      ('e', e), ('pi', pi),\n",
    "                                      ('mu1', mu1), ('mu2', mu2),\n",
    "                                      ('std1', std1), ('std2', std2),\n",
    "                                      ('rho', rho),\n",
    "                                      ('phi', rnn_model.last_phi),\n",
    "                                      ('window', rnn_model.states[-3]),\n",
    "                                      ('kappa', rnn_model.states[-2]),\n",
    "                                      ('finish', rnn_model.states[-1]),\n",
    "                                      ('zero_states', rnn_model.zero_states)]:\n",
    "                            tf.add_to_collection(*param)\n",
    "\n",
    "                with tf.name_scope('training'):\n",
    "                    steps = tf.Variable(0.)\n",
    "                    learning_rate = tf.train.exponential_decay(0.001, steps, staircase=True,\n",
    "                                                               decay_steps=10000, decay_rate=0.5)\n",
    "\n",
    "                    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "                    grad, var = zip(*optimizer.compute_gradients(loss))\n",
    "                    grad, _ = tf.clip_by_global_norm(grad, 3.)\n",
    "                    train_step = optimizer.apply_gradients(zip(grad, var), global_step=steps)\n",
    "\n",
    "                with tf.name_scope('summary'):\n",
    "                    # TODO: add more summaries\n",
    "                    summary = tf.summary.merge([\n",
    "                        tf.summary.scalar('loss', loss)\n",
    "                    ])\n",
    "\n",
    "                return namedtuple('Model', ['coordinates', 'sequence', 'reset_states', 'reset', 'loss', 'train_step',\n",
    "                                            'learning_rate', 'summary'])(\n",
    "                           coordinates, sequence, reset_states, reset, loss, train_step, learning_rate, summary\n",
    "                       )\n",
    "        train_model = create_model(generate=None)\n",
    "        _ = create_model(generate=True)  # just to create ops for generation\n",
    "\n",
    "    return graph, train_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) All set, let's train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# realtime plotting of graph\n",
    "plot_realtime = False\n",
    "if plot_realtime:\n",
    "    style.use('fivethirtyeight')\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    plt.ion()\n",
    "\n",
    "    plt.show()\n",
    "    fig.canvas.draw()\n",
    "\n",
    "def live_plot(loss_vals):\n",
    "    ax.clear()\n",
    "    ax.plot(loss_vals)\n",
    "    fig.canvas.draw()\n",
    "\n",
    "losses = []\n",
    "# train the model\n",
    "%run data_generator.ipynb\n",
    "batch_generator = DataGenerator(batch_size, seq_len)\n",
    "g, vs = create_graph(batch_generator.num_chars, batch_size,\n",
    "                     num_units=units_per_layer, lstm_layers=lstm_layers,\n",
    "                     window_mixtures=window_mixtures,\n",
    "                     output_mixtures=output_mixtures)\n",
    "graph_count = 0\n",
    "with tf.Session(graph=g) as sess:\n",
    "    model_saver = tf.train.Saver(max_to_keep=2)\n",
    "    if restore_model:\n",
    "        model_file = tf.train.latest_checkpoint(os.path.join(restore_model, 'models'))\n",
    "        mod_path = restore_model\n",
    "        epoch = int(model_file.split('-')[-1]) + 1\n",
    "        model_saver.restore(sess, model_file)\n",
    "    else:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        mod_path = model_path()\n",
    "        epoch = 0\n",
    "\n",
    "    summary_writer = tf.summary.FileWriter(mod_path, graph=g, flush_secs=10)\n",
    "    summary_writer.add_session_log(tf.SessionLog(status=tf.SessionLog.START),\n",
    "                                   global_step=epoch * batches_per_epoch)\n",
    "\n",
    "    for e in range(epoch, num_epoch):\n",
    "        print('\\nEpoch {}'.format(e))\n",
    "        for b in range(1, batches_per_epoch + 1):\n",
    "            coords, seq, reset, needed = batch_generator.next_batch()\n",
    "            if needed:\n",
    "                sess.run(vs.reset_states, feed_dict={vs.reset: reset})\n",
    "            l, s, _ = sess.run([vs.loss, vs.summary, vs.train_step],\n",
    "                               feed_dict={vs.coordinates: coords,\n",
    "                                          vs.sequence: seq})\n",
    "            losses.append(l)\n",
    "            graph_count += 1\n",
    "\n",
    "            if plot_realtime and graph_count == 1:\n",
    "                live_plot(losses)\n",
    "                graph_count = 0\n",
    "\n",
    "            summary_writer.add_summary(s, global_step=e * batches_per_epoch + b)\n",
    "            #print('\\r[{:5d}/{:5d}] loss = {}'.format(b, batches_per_epoch, l), end='')\n",
    "\n",
    "        model_saver.save(sess, os.path.join(mod_path, 'models', 'model'),\n",
    "                         global_step=e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I trained the model on Google Colab for 5-6 hours and , the model spits out negative values of losses after a certain number of ephocs and seemed to worked well so no issue with that. The code also keeps on plotting a graph in realtime as per the number of epochs so that the changes loss can be supervised with training simultaneously, just change the \"plot_realtime\" variable to true and uncomment print statement to print the losses for each epoch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
